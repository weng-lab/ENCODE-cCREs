{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74c1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import umap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81f2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_UMAP(data, neighbors, latentDim, name):\n",
    "    reducer = umap.UMAP(n_neighbors = neighbors)\n",
    "    x = reducer.fit_transform(data)\n",
    "    outputFile = open(\"UMAP.\"+str(neighbors)+\".\"+str(latentDim)+\".\"+name+\".txt\", \"w+\")\n",
    "    for entry in x:\n",
    "        print(entry[0], \"\\t\", entry[1], file=outputFile)\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cf3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15079f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f774e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encode(seq):\n",
    "    baseDict = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "    oneHotArray = []\n",
    "    seq = seq.upper()\n",
    "    seqArray = np.array(list(seq))\n",
    "    for base in seqArray:\n",
    "        code = [0, 0, 0, 0]\n",
    "        if base != \"N\":\n",
    "            code[baseDict[base]] = 1\n",
    "        else:\n",
    "            code = [0.25, 0.25, 0.25, 0.25]\n",
    "        oneHotArray.append(code)\n",
    "    return oneHotArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033897aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_Sequences(fasta, seqLength):\n",
    "    elementList = [next(fasta).lstrip(\">\").rstrip()]\n",
    "    inputData = []\n",
    "    seq=''\n",
    "    for line in fasta:\n",
    "        if \">\" not in line:\n",
    "            seq += line.rstrip()\n",
    "        else:\n",
    "            inputData.append(One_Hot_Encode(seq))\n",
    "            elementList.append(line.lstrip(\">\"))\n",
    "            seq=''\n",
    "    inputData.append(One_Hot_Encode(seq))\n",
    "    inputData = np.array(inputData)\n",
    "    inputData = inputData.reshape(inputData.shape[0], seqLength, 4)\n",
    "    return elementList, inputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce779788",
   "metadata": {},
   "outputs": [],
   "source": [
    "latentDim = 10 #int(sys.argv[1])\n",
    "seqLength = 300\n",
    "fasta=open(\"/home/moorej3/Lab/ENCODE/Encyclopedia/V7/Registry/V7-hg38/CORVUS/Input-Sequences/HCT116.PLS-ELS.fa\")\n",
    "#fasta=open(sys.argv[2])\n",
    "ccres, input = Process_Sequences(fasta, seqLength)\n",
    "fasta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd68532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           [(None, 300, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 300, 16)      656         input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 150, 16)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 2400)         0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 16)           38416       flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           170         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           170         dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_9 (Sampling)           (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 39,412\n",
      "Trainable params: 39,412\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "latentDim = 10 #int(sys.argv[1])\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(seqLength, 4))\n",
    "x = layers.Conv1D(filters=16, kernel_size=10, strides=1, activation=\"relu\", padding=\"same\")(encoder_inputs)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latentDim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latentDim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea3621c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_23 (InputLayer)        [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2400)              26400     \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 150, 16)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_13 (UpSampling (None, 300, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_24 (Conv1DT (None, 300, 16)           2576      \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_25 (Conv1DT (None, 300, 4)            196       \n",
      "=================================================================\n",
      "Total params: 29,172\n",
      "Trainable params: 29,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latentDim,))\n",
    "x = layers.Dense(16, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Dense(2400, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((150, 16))(x)\n",
    "x = layers.UpSampling1D(size=2)(x)\n",
    "x = layers.Conv1DTranspose(filters=16, kernel_size=10, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv1DTranspose(4, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "print(decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f489c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "689d598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2457/2457 [==============================] - 36s 14ms/step - loss: 5350.8482 - reconstruction_loss: 5274.9102 - kl_loss: 11.9472\n",
      "Epoch 2/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5252.8542 - reconstruction_loss: 5232.1948 - kl_loss: 8.8746\n",
      "Epoch 3/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5223.4195 - reconstruction_loss: 5211.3013 - kl_loss: 10.2558\n",
      "Epoch 4/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5221.6139 - reconstruction_loss: 5210.1929 - kl_loss: 10.1422\n",
      "Epoch 5/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5221.4925 - reconstruction_loss: 5209.7368 - kl_loss: 10.1300\n",
      "Epoch 6/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5220.5946 - reconstruction_loss: 5209.4756 - kl_loss: 10.1277\n",
      "Epoch 7/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5220.5533 - reconstruction_loss: 5209.2744 - kl_loss: 10.1199\n",
      "Epoch 8/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5219.9666 - reconstruction_loss: 5209.0454 - kl_loss: 10.0976\n",
      "Epoch 9/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5219.7305 - reconstruction_loss: 5208.8936 - kl_loss: 10.0832\n",
      "Epoch 10/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5219.9583 - reconstruction_loss: 5208.7788 - kl_loss: 10.0771\n",
      "Epoch 11/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.4798 - reconstruction_loss: 5208.6558 - kl_loss: 10.0571\n",
      "Epoch 12/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5219.6373 - reconstruction_loss: 5208.5352 - kl_loss: 10.0630\n",
      "Epoch 13/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.7299 - reconstruction_loss: 5208.4404 - kl_loss: 10.0519\n",
      "Epoch 14/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5219.3644 - reconstruction_loss: 5208.3716 - kl_loss: 10.0508\n",
      "Epoch 15/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.3062 - reconstruction_loss: 5208.3330 - kl_loss: 10.0232\n",
      "Epoch 16/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.2732 - reconstruction_loss: 5208.2905 - kl_loss: 10.0310\n",
      "Epoch 17/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5219.4519 - reconstruction_loss: 5208.1938 - kl_loss: 10.0318\n",
      "Epoch 18/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.4966 - reconstruction_loss: 5208.1357 - kl_loss: 10.0265\n",
      "Epoch 19/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.9778 - reconstruction_loss: 5208.1055 - kl_loss: 10.0067\n",
      "Epoch 20/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.7354 - reconstruction_loss: 5208.0239 - kl_loss: 10.0171\n",
      "Epoch 21/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5219.9905 - reconstruction_loss: 5207.9585 - kl_loss: 10.0166\n",
      "Epoch 22/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.3321 - reconstruction_loss: 5207.9272 - kl_loss: 10.0122\n",
      "Epoch 23/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.9762 - reconstruction_loss: 5207.8784 - kl_loss: 10.0191\n",
      "Epoch 24/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5218.5454 - reconstruction_loss: 5207.8252 - kl_loss: 10.0095\n",
      "Epoch 25/25\n",
      "2457/2457 [==============================] - 35s 14ms/step - loss: 5219.7795 - reconstruction_loss: 5207.8135 - kl_loss: 10.0041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa413192d60>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(input, epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1fb78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, _, _ = vae.encoder.predict(input)\n",
    "Run_UMAP(z_mean, 100, latentDim, \"HCT116-16CNN-16Dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc35fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
