{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74c1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import umap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81f2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_UMAP(data, neighbors, latentDim, name):\n",
    "    reducer = umap.UMAP(n_neighbors = neighbors)\n",
    "    x = reducer.fit_transform(data)\n",
    "    outputFile = open(\"UMAP.\"+str(neighbors)+\".\"+str(latentDim)+\".\"+name+\".txt\", \"w+\")\n",
    "    for entry in x:\n",
    "        print(entry[0], \"\\t\", entry[1], file=outputFile)\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cf3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15079f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f774e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encode(seq):\n",
    "    baseDict = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "    oneHotArray = []\n",
    "    seq = seq.upper()\n",
    "    seqArray = np.array(list(seq))\n",
    "    for base in seqArray:\n",
    "        code = [0, 0, 0, 0]\n",
    "        if base != \"N\":\n",
    "            code[baseDict[base]] = 1\n",
    "        else:\n",
    "            code = [0.25, 0.25, 0.25, 0.25]\n",
    "        oneHotArray.append(code)\n",
    "    return oneHotArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033897aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_Sequences(fasta, seqLength):\n",
    "    elementList = [next(fasta).lstrip(\">\").rstrip()]\n",
    "    inputData = []\n",
    "    seq=''\n",
    "    for line in fasta:\n",
    "        if \">\" not in line:\n",
    "            seq += line.rstrip()\n",
    "        else:\n",
    "            inputData.append(One_Hot_Encode(seq))\n",
    "            elementList.append(line.lstrip(\">\"))\n",
    "            seq=''\n",
    "    inputData.append(One_Hot_Encode(seq))\n",
    "    inputData = np.array(inputData)\n",
    "    inputData = inputData.reshape(inputData.shape[0], seqLength, 4)\n",
    "    return elementList, inputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce779788",
   "metadata": {},
   "outputs": [],
   "source": [
    "latentDim = 10 #int(sys.argv[1])\n",
    "seqLength = 300\n",
    "fasta=open(\"/home/moorej3/Lab/ENCODE/Encyclopedia/V7/Registry/V7-hg38/CORVUS/Input-Sequences/K562.PLS-ELS.fa\")\n",
    "#fasta=open(sys.argv[2])\n",
    "ccres, input = Process_Sequences(fasta, seqLength)\n",
    "fasta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd68532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           [(None, 300, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 300, 16)      656         input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 150, 16)      0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 2400)         0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 16)           38416       flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           170         dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           170         dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_10 (Sampling)          (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 39,412\n",
      "Trainable params: 39,412\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "latentDim = 10 #int(sys.argv[1])\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(seqLength, 4))\n",
    "x = layers.Conv1D(filters=16, kernel_size=10, strides=1, activation=\"relu\", padding=\"same\")(encoder_inputs)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latentDim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latentDim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea3621c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2400)              26400     \n",
      "_________________________________________________________________\n",
      "reshape_12 (Reshape)         (None, 150, 16)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_14 (UpSampling (None, 300, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_26 (Conv1DT (None, 300, 16)           2576      \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_27 (Conv1DT (None, 300, 4)            196       \n",
      "=================================================================\n",
      "Total params: 29,172\n",
      "Trainable params: 29,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latentDim,))\n",
    "x = layers.Dense(16, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Dense(2400, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((150, 16))(x)\n",
    "x = layers.UpSampling1D(size=2)(x)\n",
    "x = layers.Conv1DTranspose(filters=16, kernel_size=10, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv1DTranspose(4, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "print(decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f489c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "689d598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3226/3226 [==============================] - 48s 15ms/step - loss: 5376.2077 - reconstruction_loss: 5292.0430 - kl_loss: 9.6633\n",
      "Epoch 2/25\n",
      "3226/3226 [==============================] - 47s 14ms/step - loss: 5254.2677 - reconstruction_loss: 5242.8154 - kl_loss: 7.4702\n",
      "Epoch 3/25\n",
      "3226/3226 [==============================] - 47s 14ms/step - loss: 5248.4559 - reconstruction_loss: 5240.5034 - kl_loss: 7.0000\n",
      "Epoch 4/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.5583 - reconstruction_loss: 5239.9512 - kl_loss: 6.8088\n",
      "Epoch 5/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.9458 - reconstruction_loss: 5239.6108 - kl_loss: 6.7687\n",
      "Epoch 6/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.9044 - reconstruction_loss: 5239.3975 - kl_loss: 6.7404\n",
      "Epoch 7/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.1819 - reconstruction_loss: 5239.3003 - kl_loss: 6.7421\n",
      "Epoch 8/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.9779 - reconstruction_loss: 5239.1753 - kl_loss: 6.7130\n",
      "Epoch 9/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.4089 - reconstruction_loss: 5239.0581 - kl_loss: 6.7130\n",
      "Epoch 10/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.3908 - reconstruction_loss: 5238.9937 - kl_loss: 6.6994\n",
      "Epoch 11/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.4229 - reconstruction_loss: 5238.9360 - kl_loss: 6.7000\n",
      "Epoch 12/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.3101 - reconstruction_loss: 5238.8643 - kl_loss: 6.6904\n",
      "Epoch 13/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.4454 - reconstruction_loss: 5238.7832 - kl_loss: 6.6879\n",
      "Epoch 14/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.0347 - reconstruction_loss: 5238.7871 - kl_loss: 6.6794\n",
      "Epoch 15/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.8759 - reconstruction_loss: 5238.7148 - kl_loss: 6.6828\n",
      "Epoch 16/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.2709 - reconstruction_loss: 5238.6890 - kl_loss: 6.6738\n",
      "Epoch 17/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.9738 - reconstruction_loss: 5238.6406 - kl_loss: 6.6650\n",
      "Epoch 18/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.8360 - reconstruction_loss: 5238.6118 - kl_loss: 6.6585\n",
      "Epoch 19/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.6139 - reconstruction_loss: 5238.5923 - kl_loss: 6.6647\n",
      "Epoch 20/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.9881 - reconstruction_loss: 5238.5659 - kl_loss: 6.6556\n",
      "Epoch 21/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.4372 - reconstruction_loss: 5238.5278 - kl_loss: 6.6573\n",
      "Epoch 22/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5247.1666 - reconstruction_loss: 5238.5054 - kl_loss: 6.6392\n",
      "Epoch 23/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.2921 - reconstruction_loss: 5238.4810 - kl_loss: 6.6445\n",
      "Epoch 24/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.5841 - reconstruction_loss: 5238.4497 - kl_loss: 6.6388\n",
      "Epoch 25/25\n",
      "3226/3226 [==============================] - 46s 14ms/step - loss: 5246.1415 - reconstruction_loss: 5238.4282 - kl_loss: 6.6380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa4142a6fa0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(input, epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1fb78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, _, _ = vae.encoder.predict(input)\n",
    "Run_UMAP(z_mean, 100, latentDim, \"K562-16CNN-16Dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc35fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
