{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74c1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import umap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f81f2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_UMAP(data, neighbors, latentDim, name):\n",
    "    reducer = umap.UMAP(n_neighbors = neighbors)\n",
    "    x = reducer.fit_transform(data)\n",
    "    outputFile = open(\"UMAP.\"+str(neighbors)+\".\"+str(latentDim)+\".\"+name+\".txt\", \"w+\")\n",
    "    for entry in x:\n",
    "        print(entry[0], \"\\t\", entry[1], file=outputFile)\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cf3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15079f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f774e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encode(seq):\n",
    "    baseDict = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "    oneHotArray = []\n",
    "    seq = seq.upper()\n",
    "    seqArray = np.array(list(seq))\n",
    "    for base in seqArray:\n",
    "        code = [0, 0, 0, 0]\n",
    "        if base != \"N\":\n",
    "            code[baseDict[base]] = 1\n",
    "        else:\n",
    "            code = [0.25, 0.25, 0.25, 0.25]\n",
    "        oneHotArray.append(code)\n",
    "    return oneHotArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033897aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_Sequences(fasta, seqLength):\n",
    "    elementList = [next(fasta).lstrip(\">\").rstrip()]\n",
    "    inputData = []\n",
    "    seq=''\n",
    "    for line in fasta:\n",
    "        if \">\" not in line:\n",
    "            seq += line.rstrip()\n",
    "        else:\n",
    "            inputData.append(One_Hot_Encode(seq))\n",
    "            elementList.append(line.lstrip(\">\"))\n",
    "            seq=''\n",
    "    inputData.append(One_Hot_Encode(seq))\n",
    "    inputData = np.array(inputData)\n",
    "    inputData = inputData.reshape(inputData.shape[0], seqLength, 4)\n",
    "    return elementList, inputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce779788",
   "metadata": {},
   "outputs": [],
   "source": [
    "latentDim = 10 #int(sys.argv[1])\n",
    "seqLength = 300\n",
    "fasta=open(\"/home/moorej3/Lab/ENCODE/Encyclopedia/V7/Registry/V7-hg38/CORVUS/Input-Sequences/HepG2.PLS-ELS.fa\")\n",
    "#fasta=open(sys.argv[2])\n",
    "ccres, input = Process_Sequences(fasta, seqLength)\n",
    "fasta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd68532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 300, 4)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 300, 16)      656         input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 150, 16)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 2400)         0           max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 16)           38416       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           170         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           170         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sampling_8 (Sampling)           (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 39,412\n",
      "Trainable params: 39,412\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "latentDim = 10 #int(sys.argv[1])\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(seqLength, 4))\n",
    "x = layers.Conv1D(filters=16, kernel_size=10, strides=1, activation=\"relu\", padding=\"same\")(encoder_inputs)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latentDim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latentDim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea3621c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2400)              26400     \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         (None, 150, 16)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_12 (UpSampling (None, 300, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_22 (Conv1DT (None, 300, 16)           2576      \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_23 (Conv1DT (None, 300, 4)            196       \n",
      "=================================================================\n",
      "Total params: 29,172\n",
      "Trainable params: 29,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latentDim,))\n",
    "x = layers.Dense(16, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Dense(2400, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((150, 16))(x)\n",
    "x = layers.UpSampling1D(size=2)(x)\n",
    "x = layers.Conv1DTranspose(filters=16, kernel_size=10, activation=\"relu\", strides=1, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv1DTranspose(4, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "print(decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f489c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "689d598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2297/2297 [==============================] - 36s 15ms/step - loss: 5390.1260 - reconstruction_loss: 5305.7124 - kl_loss: 10.7639\n",
      "Epoch 2/25\n",
      "2297/2297 [==============================] - 33s 15ms/step - loss: 5263.9493 - reconstruction_loss: 5246.2520 - kl_loss: 9.8307\n",
      "Epoch 3/25\n",
      "2297/2297 [==============================] - 33s 15ms/step - loss: 5251.2666 - reconstruction_loss: 5241.2441 - kl_loss: 8.8723\n",
      "Epoch 4/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5250.3767 - reconstruction_loss: 5240.3516 - kl_loss: 8.5511\n",
      "Epoch 5/25\n",
      "2297/2297 [==============================] - 33s 15ms/step - loss: 5250.2902 - reconstruction_loss: 5239.8877 - kl_loss: 8.3847\n",
      "Epoch 6/25\n",
      "2297/2297 [==============================] - 34s 15ms/step - loss: 5248.3323 - reconstruction_loss: 5239.5713 - kl_loss: 8.2952\n",
      "Epoch 7/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5249.3067 - reconstruction_loss: 5239.3003 - kl_loss: 8.2427\n",
      "Epoch 8/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5248.2163 - reconstruction_loss: 5239.1655 - kl_loss: 8.2201\n",
      "Epoch 9/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5248.3602 - reconstruction_loss: 5238.9702 - kl_loss: 8.2108\n",
      "Epoch 10/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5248.0162 - reconstruction_loss: 5238.8413 - kl_loss: 8.1975\n",
      "Epoch 11/25\n",
      "2297/2297 [==============================] - 33s 15ms/step - loss: 5247.5297 - reconstruction_loss: 5238.7358 - kl_loss: 8.1821\n",
      "Epoch 12/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5247.6559 - reconstruction_loss: 5238.6118 - kl_loss: 8.1728\n",
      "Epoch 13/25\n",
      "2297/2297 [==============================] - 33s 15ms/step - loss: 5247.4525 - reconstruction_loss: 5238.5049 - kl_loss: 8.1895\n",
      "Epoch 14/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5247.5483 - reconstruction_loss: 5238.4443 - kl_loss: 8.1755\n",
      "Epoch 15/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5248.2207 - reconstruction_loss: 5238.3979 - kl_loss: 8.1689\n",
      "Epoch 16/25\n",
      "2297/2297 [==============================] - 33s 15ms/step - loss: 5247.6709 - reconstruction_loss: 5238.3403 - kl_loss: 8.1699\n",
      "Epoch 17/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5247.8272 - reconstruction_loss: 5238.2363 - kl_loss: 8.1504\n",
      "Epoch 18/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5247.4997 - reconstruction_loss: 5238.2222 - kl_loss: 8.1594\n",
      "Epoch 19/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5248.3545 - reconstruction_loss: 5238.1680 - kl_loss: 8.1556\n",
      "Epoch 20/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5247.3363 - reconstruction_loss: 5238.1021 - kl_loss: 8.1523\n",
      "Epoch 21/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5247.1793 - reconstruction_loss: 5238.0781 - kl_loss: 8.1471\n",
      "Epoch 22/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5247.3340 - reconstruction_loss: 5238.0415 - kl_loss: 8.1504\n",
      "Epoch 23/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5247.3688 - reconstruction_loss: 5237.9883 - kl_loss: 8.1551\n",
      "Epoch 24/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5248.0308 - reconstruction_loss: 5237.9839 - kl_loss: 8.1498\n",
      "Epoch 25/25\n",
      "2297/2297 [==============================] - 33s 14ms/step - loss: 5247.3234 - reconstruction_loss: 5237.9785 - kl_loss: 8.1436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa411e56a60>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(input, epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1fb78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, _, _ = vae.encoder.predict(input)\n",
    "Run_UMAP(z_mean, 100, latentDim, \"HepG2-16CNN-16Dense\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
